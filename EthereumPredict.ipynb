{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq4IUSytPQ9x",
        "outputId": "b30e9234-7906-4d32-cc2d-8b3f83b045e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7fbbc53f3820>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBNKtErgR5Af"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.client import device_lib \n",
        "from keras import layers\n",
        "from keras import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Layer\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import MultiHeadAttention\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU\n",
        "from keras.layers import LayerNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Flatten\n",
        "from keras.backend import dot\n",
        "from keras.backend import sin\n",
        "from keras.backend import reshape\n",
        "from keras.backend import flatten\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "\n",
        "def retrieve_data(filename):\n",
        "    # Remove redundant features if they are found\n",
        "    data = pd.read_csv(filename) \n",
        "    data = data.drop(['open-time','close-time'], axis=1, errors='ignore')\n",
        "    # Variables: ['open-price', 'high-price','low-price', 'close-price', 'volume','quote-asset-volume', 'number-of-trades', 'taker-buy-base-asset-volume', 'taker-buy-quote-asset-volume']\n",
        "    return data.to_numpy()\n",
        "\n",
        "\n",
        "def scaleAndFilterData(data, testsize=0.1):\n",
        "    # Split the train and test data\n",
        "    split_index = math.floor((1-testsize)*len(data))\n",
        "    traindata = data[:split_index]\n",
        "    testdata = data[split_index:]\n",
        "\n",
        "    # Perform minmax-scaling (separately for train and test data)\n",
        "    traindata_scaled = minmax_scale(traindata)\n",
        "    testdata_scaled = minmax_scale(testdata)\n",
        "    \n",
        "    return traindata, testdata, traindata_scaled, testdata_scaled\n",
        "\n",
        "\n",
        "def convertDataFormat(data, timeframe:int=24): #if want day timeframe to train, put in 24 for timeframe, training on 23 hours and predicting close price of the 24th hour\n",
        "    iterations = data.shape[0]-timeframe+1\n",
        "    x_data = np.zeros((iterations,timeframe-1,data.shape[1]))\n",
        "    y_data = np.zeros((iterations))\n",
        "    for i in range(iterations):\n",
        "        x_data[i] = data[i:i+timeframe-1,:]\n",
        "        y_data[i] = data[i+timeframe-1, 3] # 'close-price'\n",
        "    return x_data, y_data\n",
        "\n",
        "\n",
        "class multi_Head_Attention_encoder(Layer): #this only builds an encoder\n",
        "\n",
        "    def __init__(self, dropout, amount_of_heads, size_of_head,output_dim, **kwargs):\n",
        "        super(multi_Head_Attention_encoder,self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.amount_of_heads= amount_of_heads \n",
        "        self.size_of_head = size_of_head\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.multi_Attention = MultiHeadAttention(key_dim=self.size_of_head, num_heads=self.amount_of_heads, value_dim= self.size_of_head, dropout=self.dropout,   attention_axes= (1,2))\n",
        "        self.norm_att = LayerNormalization() #can set epsiolon, standard = 0.001\n",
        "        self.feed_forward = LSTM(self.output_dim,activation='tanh', use_bias=True, return_sequences=True) #must be as same as features\n",
        "        self.feed_forward_2 = LSTM(self.output_dim*4,activation='tanh', use_bias=True, return_sequences=True) #must be as same as features\n",
        "        self.feed_forward_3 = LSTM(self.output_dim,activation='tanh', use_bias=True, return_sequences=True) #must be as same as features\n",
        "        self.dropout_ff = Dropout(self.dropout)\n",
        "        self.norm_ff = LayerNormalization() #can set epsilon, standard = 0.001\n",
        "        super(multi_Head_Attention_encoder, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, training = None):\n",
        "        attention_output = self.multi_Attention(inputs, inputs, training = training)\n",
        "        normalization_output = self.norm_att(inputs+attention_output)\n",
        "        forward = self.feed_forward(normalization_output)\n",
        "        forward = self.dropout_ff(forward, training = training)\n",
        "        forward = self.feed_forward_2(forward)\n",
        "        forward = self.dropout_ff(forward, training = training)\n",
        "        forward = self.feed_forward_3(forward)\n",
        "        forward = self.norm_ff(forward+normalization_output)\n",
        "        return forward\n",
        "    \n",
        "class multi_head_attention_decoder(Layer):\n",
        "\n",
        "    def __init__(self, dropout, amount_of_heads, size_of_head, output_dim, **kwargs):\n",
        "        super(multi_head_attention_decoder,self).__init__(**kwargs)\n",
        "        self.dropout = dropout\n",
        "        self.amount_of_heads= amount_of_heads \n",
        "        self.size_of_head = size_of_head\n",
        "        self.output_dim = output_dim\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.multi_Attention_1 = MultiHeadAttention(key_dim=self.size_of_head, num_heads=self.amount_of_heads, value_dim= self.size_of_head, dropout=self.dropout)\n",
        "        self.norm_att_1 = LayerNormalization() #can set epsiolon, standard = 0.001\n",
        "        self.multi_Attention = MultiHeadAttention(key_dim=self.size_of_head, num_heads=self.amount_of_heads, value_dim= self.size_of_head, dropout=self.dropout,  attention_axes= (1,2))\n",
        "        self.norm_att = LayerNormalization() #can set epsiolon, standard = 0.001\n",
        "        self.feed_forward = LSTM(self.output_dim,activation='tanh', return_sequences=True) #must be as same as features\n",
        "        self.feed_forward_2 = LSTM(self.output_dim*4,activation='tanh', use_bias=True, return_sequences=True) #must be as same as features\n",
        "        self.feed_forward_3 = LSTM(self.output_dim,activation='tanh', use_bias=True, return_sequences=True) #must be as same as features\n",
        "        self.dropout_ff = Dropout(self.dropout)\n",
        "        self.norm_ff = LayerNormalization()\n",
        "        super(multi_head_attention_decoder, self).build(input_shape)   \n",
        "\n",
        "    def call(self, inputs, training = None):\n",
        "        encoder_input, target = inputs\n",
        "        attention_output_1 = self.multi_Attention(query =target, key = target, value = target, training = training)\n",
        "        norm_output_1 = self.norm_att(attention_output_1+target)\n",
        "        attention_output = self.multi_Attention(query =encoder_input, key = norm_output_1, value = norm_output_1, training = training)\n",
        "        norm_output = self.norm_att(attention_output+norm_output_1)  \n",
        "        forward =self.feed_forward(norm_output)\n",
        "        forward = self.dropout_ff(forward, training = training)\n",
        "        forward = self.feed_forward_2(forward)\n",
        "        forward = self.dropout_ff(forward, training = training)\n",
        "        forward = self.feed_forward_3(forward)\n",
        "        forward = self.norm_ff(forward+norm_output)\n",
        "        return forward\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, dim_list, dropout,  **kwargs):\n",
        "        super(Linear,self).__init__(**kwargs)\n",
        "        self.dim_list = dim_list\n",
        "        self.dropout = dropout\n",
        "        self.dense_layers = []\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        for i in self.dim_list:\n",
        "            self.dense_layers.append(Dense(i,activation='linear')  )\n",
        "        self.dropout_ff = Dropout(0.004)\n",
        "        super(Linear, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, training = None):\n",
        "        forward = inputs\n",
        "        for layer in self.dense_layers[:-1]:\n",
        "            forward = layer(forward)\n",
        "            forward = self.dropout_ff(forward, training = training)\n",
        "        last_layer = self.dense_layers[-1]\n",
        "        return last_layer(forward)\n",
        "\n",
        "class Time2Vec(Layer):\n",
        "\n",
        "    def __init__(self, k, **kwargs):\n",
        "        self.k = k\n",
        "        super(Time2Vec, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        #times the input, so amount of rows of w must be equal to amount of colums of input\n",
        "        self.w = self.add_weight(shape=(input_shape[-1], self.k), initializer='uniform',trainable=True)#weights if i>0 \n",
        "        self.fi = self.add_weight(shape=(input_shape[1],self.k),initializer='uniform',trainable=True)#weights if i>0     \n",
        "        self.w0 = self.add_weight(shape=(input_shape[-1],1),initializer='uniform', trainable=True) #weights for i=0\n",
        "        self.fi0 = self.add_weight(shape=(input_shape[1],1),initializer='uniform', trainable=True) #weights for i=0     \n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        first_entry = tf.matmul(inputs,self.w0) +self.fi0\n",
        "        rest_of_time_vector = tf.matmul(inputs,self.w) + self.fi\n",
        "        output = sin(rest_of_time_vector)\n",
        "        return_value = concatenate([first_entry,output], -1)\n",
        "        #also flatten output? but what is the point then of the timedistributed layer\n",
        "        return return_value\n",
        "\n",
        "class BuildModel(Model):\n",
        "\n",
        "    def __init__(self, k, amount_of_layers_encoder, amount_of_layers_decoder, dropout,amount_of_heads,size_of_head, **kwargs): #ff dim must be equal to amount of features to work\n",
        "        #k=5? because hour, day, week,month, year to identify time? dimension for each thing\n",
        "        super().__init__(**kwargs)\n",
        "        self.time2vec_layer_encode = Time2Vec(k)\n",
        "        self.time2vec_layer_decode = Time2Vec(k)\n",
        "        self.attention_layers_encoder = []\n",
        "        self.attention_layers_decoder = []\n",
        "        for _ in range(amount_of_layers_encoder):\n",
        "            self.attention_layers_encoder.append(multi_Head_Attention_encoder(dropout,amount_of_heads,size_of_head, output_dim = k+10))\n",
        "        for _ in range(amount_of_layers_decoder):\n",
        "            self.attention_layers_decoder.append(multi_head_attention_decoder(dropout,amount_of_heads,size_of_head, output_dim = k+10))\n",
        "        self.linear_layer = Linear(dim_list= [1], dropout= dropout)  \n",
        "        self.flatten = Flatten()    \n",
        "        self.concatenate = Concatenate()\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        input,target = inputs\n",
        "        #timevector embedding encoder input\n",
        "        time_vector_embedded_encode =self.time2vec_layer_encode(input)\n",
        "        time_vector_embedded_encode = self.concatenate([input,time_vector_embedded_encode])\n",
        "        #timevector embedding decoder input\n",
        "        time_vector_embedded_decode =self.time2vec_layer_decode(target)\n",
        "        time_vector_embedded_decode = self.concatenate([target,time_vector_embedded_decode])\n",
        "        #pass input forward trhough first all encoder layers\n",
        "        attention_encoder_input = time_vector_embedded_encode\n",
        "        for layer_encode in self.attention_layers_encoder:\n",
        "            attention_encoder_input = layer_encode(attention_encoder_input)\n",
        "        attention_decoder_input = time_vector_embedded_decode\n",
        "        for layer_decoder in self.attention_layers_decoder:\n",
        "            attention_decoder_input = layer_decoder((attention_encoder_input, attention_decoder_input))\n",
        "        \n",
        "        flattened_output = self.flatten(attention_decoder_input)\n",
        "        output = self.linear_layer(flattened_output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def grid_search_transformer(x_train, target_train, y_train, x_val, target_val, y_val, epochs, params):\n",
        "    layers = params['layers']\n",
        "    k_vals = params['k']\n",
        "    nr_heads = params['nr_heads']\n",
        "    size_heads = params['size_heads']\n",
        "    dropout = params['dropout']\n",
        "    predictions = {}\n",
        "    for la in layers:\n",
        "      for he in nr_heads:\n",
        "        for siz in size_heads:\n",
        "          for dr in dropout:\n",
        "            for k in k_vals:\n",
        "              print('Building model with: layers' + str(la)\n",
        "              + ', nr_heads=' + str(he)\n",
        "              + ', size_heads=' + str(siz)\n",
        "              + ', dropout=' + str(dr)\n",
        "              + ', k=' + str(k) )\n",
        "              model = BuildModel(k=k, amount_of_layers_encoder=la, \n",
        "                                  amount_of_layers_decoder=la, dropout=dr,\n",
        "                                  amount_of_heads=he, size_of_head=siz)\n",
        "              model.compile(optimizer = \n",
        "                            keras.optimizers.Adam(learning_rate = 1e-3), \n",
        "                            loss = keras.losses.mse)\n",
        "              model.fit((x_train, target_train), y_train,\n",
        "                        batch_size=64, epochs=epochs, verbose=1,\n",
        "                        validation_data=((x_val, target_val), y_val))\n",
        "              y_pred = model.predict((x_val, target_val))\n",
        "\n",
        "              predictions[\n",
        "                      'la_' + str(la) \n",
        "              + '_' + 'he_' + str(he) \n",
        "              + '_' + 'siz_' + str(siz) \n",
        "              + '_' + 'dr_' + str(dr) \n",
        "              + '_' + 'k_' + str(k) \n",
        "              ] = y_pred\n",
        "              print('mean abs perc error of: ' \n",
        "                    + str(mean_absolute_percentage_error(y_pred.flatten(), y_val)))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def grid_search_rnn(x_train, y_train, x_val, y_val, epochs, params):\n",
        "    dropout = params['dropout']\n",
        "    predictions = {}\n",
        "    for dr in dropout:\n",
        "      print('Building model with: dropout=' + str(dr))\n",
        "      model = Sequential()\n",
        "      model.add(GRU(64, input_shape=(x_train.shape[1], x_train.shape[2]), \n",
        "                    return_sequences=True, dropout=dr))\n",
        "      model.add(GRU(16, return_sequences=False))\n",
        "      model.add(Dense(units=1))\n",
        "      model.compile(optimizer = \n",
        "                    keras.optimizers.Adam(learning_rate = 1e-3), \n",
        "                    loss = keras.losses.mse)\n",
        "      model.fit(x_train, y_train,\n",
        "                batch_size=64, epochs=epochs, verbose=1,\n",
        "                validation_data=(x_val, y_val))\n",
        "      y_pred = model.predict(x_val)\n",
        "\n",
        "      predictions['dr_' + str(dr)] = y_pred\n",
        "      print('mean abs perc error of: ' \n",
        "            + str(mean_absolute_percentage_error(y_pred.flatten(), y_val)))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def calc_best_params_grid(grid, labels_true):\n",
        "    best_score = 10000\n",
        "    best_labels = []\n",
        "    best_params = None\n",
        "    for params, labels_pred in grid.items():\n",
        "        current_score = mean_absolute_percentage_error(labels_pred.flatten(), labels_true)\n",
        "        grid[params] = [labels_pred, current_score]\n",
        "        if current_score < best_score:\n",
        "            best_score = current_score\n",
        "            best_params = params\n",
        "            best_labels = labels_pred\n",
        "\n",
        "    params = best_params.split('_')[::2]\n",
        "    values = best_params.split('_')[1::2]\n",
        "    param_dict, i = {}, 0\n",
        "    for param in params:\n",
        "        param_dict[param] = values[i]\n",
        "        i += 1\n",
        "    return param_dict, best_score, best_labels, grid\n",
        "\n",
        "\n",
        "def reverseMinMaxScaling(y_scaled, input_min:float, input_max:float) -> float:\n",
        "    # Reverse the minmax-scaling to convert prediction values to USD\n",
        "\n",
        "    # a is scaled, b is USD\n",
        "    # a = (b - input_min) / (input_max - input_min) # Minmax-scaling formula\n",
        "    # a = (b - input_min) / (input_max - input_min) \n",
        "    # a * (input_max - input_min) = b - input_min\n",
        "    # b = a * (input_max - input_min) + input_min\n",
        "    return y_scaled * (input_max - input_min) + input_min\n",
        "\n",
        "\n",
        "def plotPrediction(y_true, y_pred, timeframe:int, model_name:str) -> None:\n",
        "    plt.plot(y_true)\n",
        "    plt.plot(y_pred)\n",
        "    if str(timeframe)[-1] == '1': plt.title(f'True vs. predicted {timeframe}st hour Ethereum value')\n",
        "    elif str(timeframe)[-1] == '2': plt.title(f'True vs. predicted {timeframe}nd hour Ethereum value')\n",
        "    elif str(timeframe)[-1] == '3': plt.title(f'True vs. predicted {timeframe}rd hour Ethereum value')\n",
        "    else: plt.title(f'True vs. {model_name}-predicted {timeframe}th hour Ethereum value')\n",
        "    plt.xlabel('Prediction number')\n",
        "    plt.ylabel('Value (USD)')\n",
        "    plt.legend(['True', 'Predicted'])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plotAbsPctError(y_true, y_pred, timeframe:int, model_name:str) -> list: # Also returns the top 1% outliers as list of tuples, (prediction index, APE)\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    ape = abs(y_true-y_pred)/y_true*100\n",
        "    plt.scatter(range(len(ape)), ape)\n",
        "    plt.title(f'Absolute Percentage Error per {model_name} prediction (timeframe = {timeframe})')\n",
        "    plt.xlabel('Prediction number')\n",
        "    plt.ylabel('Absolute Percentage Error')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Model = {model_name}\\tTimeframe = {timeframe}\\n')\n",
        "    print(f'Median APE: {np.median(ape)}')\n",
        "    print(f'Mean APE: {np.mean(ape)}\\n')\n",
        "\n",
        "    outliercount = math.ceil(0.01*len(ape))\n",
        "    outlier_index_tuples = sorted(enumerate(ape), reverse=True, key=lambda x: x[1])[:outliercount]\n",
        "\n",
        "    print(f'Top 1% (={outliercount}) outliers (prediction index, APE): {outlier_index_tuples}')\n",
        "    return outlier_index_tuples\n",
        "\n",
        "\n",
        "def plot_predictions(y_true, y_pred, y_original):\n",
        "    y_pred_reverse = reverseMinMaxScaling(y_pred, y_original.min(), y_original.max())\n",
        "    plotPrediction(y_original, y_pred_reverse, 24, \"RNN\")\n",
        "\n",
        "\n",
        "def retrieve_data_create_train_test_val_target():\n",
        "    # Retrieve the data, \n",
        "    filename = 'dataset.csv'\n",
        "    data = retrieve_data(filename)\n",
        "\n",
        "    # Then split and scale according to min_max scaling\n",
        "    split_size = 0.1\n",
        "    train_val_data_orig, test_data_orig, train_val_data_scaled, test_data_scaled = scaleAndFilterData(data, split_size)\n",
        "    train_data_orig, val_data_orig, train_data_scaled, val_data_scaled = scaleAndFilterData(train_val_data_orig, split_size)\n",
        "\n",
        "    # Create extra data according to a sliding frame with the timeframe and split into x and y\n",
        "    timeframe = 24\n",
        "    x_train, y_train = convertDataFormat(train_data_scaled)\n",
        "    x_val, y_val = convertDataFormat(val_data_scaled)\n",
        "\n",
        "    x_trainval, y_trainval = convertDataFormat(train_val_data_scaled)\n",
        "    x_test, y_test = convertDataFormat(test_data_scaled)\n",
        "\n",
        "    # Create target data for the transformer\n",
        "    target_train = x_train[:,-1,:]\n",
        "    target_train = np.reshape(target_train,(len(target_train),1,9))\n",
        "    target_val = x_val[:,-1,:]\n",
        "    target_val = np.reshape(target_val,(len(target_val),1,9))\n",
        "\n",
        "    target_trainval = x_trainval[:,-1,:]\n",
        "    target_trainval = np.reshape(target_trainval,(len(target_trainval),1,9))\n",
        "    target_test = x_test[:,-1,:]\n",
        "    target_test = np.reshape(target_test,(len(target_test),1,9))\n",
        "\n",
        "    final_data = {\n",
        "        'data_raw' : data,\n",
        "        'train_val_data_orig'     : train_val_data_orig,\n",
        "        'train_val_data_scaled'   : train_val_data_scaled,\n",
        "        'test_data_orig'          : test_data_orig,\n",
        "        'test_data_scaled'        : test_data_scaled,\n",
        "        'train_data_orig'         : train_data_orig,\n",
        "        'train_data_scaled'       : train_data_scaled,\n",
        "        'val_data_orig'           : val_data_orig,\n",
        "        'val_data_scaled'         : val_data_scaled,\n",
        "        'x_train'                 : x_train,\n",
        "        'y_train'                 : y_train,\n",
        "        'x_val'                   : x_val,\n",
        "        'y_val'                   : y_val,\n",
        "        'x_trainval'              : x_trainval,\n",
        "        'y_trainval'              : y_trainval,\n",
        "        'x_test'                  : x_test,\n",
        "        'y_test'                  : y_test,\n",
        "        'target_train'            : target_train,\n",
        "        'target_val'              : target_val,\n",
        "        'target_trainval'         : target_trainval,\n",
        "        'target_test'             : target_test\n",
        "                  }\n",
        "    return final_data\n",
        "\n",
        "\n",
        "def grid_search_transformer_and_calc_best(data, epochs_grid, params):\n",
        "    # Perform grid search to find the best parameters\n",
        "    grid_tr = grid_search_transformer(data['x_train'], data['target_train'], \n",
        "                                      data['y_train'], data['x_val'], data['target_val'], \n",
        "                                      data['y_val'], epochs_grid, params)\n",
        "    best_params_vals_tr = calc_best_params_grid(grid_tr, data['y_val'])\n",
        "    bp_tr = best_params_vals_tr[0]\n",
        "    print('best params: '+ str(bp_tr))\n",
        "    print('with mean abs percentage error of: '+ str(best_params_vals_tr[1]))\n",
        "    return {'grid_tr' : grid_tr, 'best_parms_vals' : best_params_vals_tr}\n",
        "\n",
        "\n",
        "def grid_search_rnn_and_calc_best(data, epochs, params):\n",
        "    # Perform grid search to find the best parameters\n",
        "    grid_rnn = grid_search_rnn(data['x_train'], data['y_train'], data['x_val'],\n",
        "                           data['y_val'], epochs, params)\n",
        "    best_params_vals_rnn = calc_best_params_grid(grid_rnn, data['y_val'])\n",
        "    bp = best_params_vals_rnn[0]\n",
        "    print('best params: '+ str(bp))\n",
        "    print('with mean abs perc error of: '+ str(best_params_vals_rnn[1]))\n",
        "    return {'grid_rnn' : grid_rnn, 'best_parms_vals' : best_params_vals_rnn}\n",
        "\n",
        "\n",
        "def build_best_transformer(data, epochs, params ,tpu = False):\n",
        "    # Build the model with the best parameters\n",
        "    la = params['la']\n",
        "    head = params['he']\n",
        "    size = params['siz']\n",
        "    dr = params['dr']\n",
        "    k = params['k']\n",
        "    \n",
        "    model_tr = BuildModel(k=k, amount_of_layers_encoder=la, \n",
        "                          amount_of_layers_decoder=la, dropout=dr,\n",
        "                      amount_of_heads=head, size_of_head=size)\n",
        " \n",
        "    model_tr.compile(optimizer \n",
        "                     = keras.optimizers.Adam(learning_rate = 1e-3,), \n",
        "                     loss=keras.losses.mse)\n",
        "    \n",
        "    if tpu:\n",
        "      device_name = os.environ['COLAB_TPU_ADDR']\n",
        "      TPU_ADDRESS = 'grpc://' + device_name\n",
        "      model_tr = tf.contrib.tpu.keras_to_tpu_model(model_tr,\n",
        "      strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "    \n",
        "    model_tr.fit((data['x_trainval'], data['target_trainval']), data['y_trainval'],\n",
        "                 batch_size=64, epochs=epochs, verbose=1)\n",
        "    \n",
        "    y_pred_tr = model_tr.predict((data['x_test'], data['target_test']))\n",
        "\n",
        "    print('Final mean abs perc error of:' + \n",
        "          str(mean_absolute_percentage_error(y_pred_tr.flatten(), data['y_test'])))\n",
        "    \n",
        "    return y_pred_tr\n",
        "    \n",
        "\n",
        "def build_best_rnn(data, epochs, params):\n",
        "    # Build the model with the best parameters\n",
        "    model_rnn = Sequential()\n",
        "\n",
        "    model_rnn.add(GRU(64, input_shape=(data['x_trainval'].shape[1], \n",
        "                                       data['x_trainval'].shape[2]), \n",
        "                  return_sequences=True, dropout=float(params['dr'])))\n",
        "    model_rnn.add(GRU(16, return_sequences=False))\n",
        "    model_rnn.add(Dense(units=1))\n",
        "\n",
        "    model_rnn.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-3), \n",
        "                      loss =keras.losses.mse)\n",
        "    \n",
        "    model_rnn.fit(data['x_trainval'], data['y_trainval'], \n",
        "                  batch_size=64, epochs=epochs, verbose=1)\n",
        "\n",
        "    y_pred_rnn = model_rnn.predict(data['x_test'])\n",
        "\n",
        "    print('Final mean abs perc error of:' + \n",
        "          str(mean_absolute_percentage_error(y_pred_rnn.flatten(), data['y_test'])))\n",
        "    \n",
        "    return y_pred_rnn\n",
        "    \n",
        "\n",
        "def split_train_test(data, testsize=0.1):\n",
        "    split_index = math.floor((1-testsize)*len(data))\n",
        "    train_data = data[:split_index]\n",
        "    test_data = data[split_index:]\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "def scale_data_y(data):\n",
        "    data_reshaped = data.reshape(-1, 1)\n",
        "    scaler = MinMaxScaler().fit(data_reshaped) \n",
        "    data_scaled = scaler.transform(data_reshaped)\n",
        "    return data_scaled, scaler\n",
        "\n",
        "def reverse_predicted_labels(y_pred):\n",
        "    # retrieve data\n",
        "    filename = 'dataset.csv'\n",
        "    data = retrieve_data(filename)\n",
        "\n",
        "    # split data in train, val and test\n",
        "    split_size = 0.1\n",
        "    train_val_unscaled, test_unscaled = split_train_test(data, split_size)\n",
        "\n",
        "    # create timeframes \n",
        "    x_test, y_test = convertDataFormat(test_unscaled)\n",
        "    y_test_scaled, y_test_scaler = scale_data_y(y_test)\n",
        "    y_test_reversed = y_test_scaler.inverse_transform(y_test_scaled.reshape(-1, 1))\n",
        "    y_pred_reversed = y_test_scaler.inverse_transform(y_pred.reshape(-1,1))\n",
        "\n",
        "    return y_test, y_pred_reversed\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred_reversed):\n",
        "    r2 = r2_score(y_true, y_pred_reversed)\n",
        "    mse = mean_squared_error(y_true, y_pred_reversed)\n",
        "    rmse = math.sqrt(mean_squared_error(y_true, y_pred_reversed))\n",
        "    mae = mean_absolute_error(y_true, y_pred_reversed)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred_reversed)\n",
        "\n",
        "    print('r2: ' + str(r2))\n",
        "    print('mse: ' + str(mse))\n",
        "    print('rmse: ' + str(rmse))\n",
        "    print('mae: ' + str(mae))\n",
        "    print('mape: ' + str(mape))\n",
        "    \n",
        "    return r2, mse, rmse, mae, mape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roIWmdE9cUXk"
      },
      "outputs": [],
      "source": [
        "data = retrieve_data_create_train_test_val_target()\n",
        "\n",
        "# Grid search transformer\n",
        "epochs_grid = 5\n",
        "params_tr = {\n",
        "    'layers' : [3],               # 3, 5\n",
        "    'nr_heads' : [8],             # 8, 12\n",
        "    'size_heads' : [11],          # 11, 23\n",
        "    'dropout' : [0.01],           # 0.03, 0.01 ???0.005, 0.001\n",
        "    'k' : [9]                     # 9, 12\n",
        "                                  # TOTAAL = 2 * 2^2 * 2 * 2 * 2 * 2 = 128\n",
        "}\n",
        "\n",
        "grid_best_params_tr = grid_search_transformer_and_calc_best(data, epochs_grid, params_tr)\n",
        "\n",
        "# Build best transformer\n",
        "epochs = 50\n",
        "params_best = {\n",
        "    'la' : 5,               # 3, 5\n",
        "    'he' : 12,             # 8, 12\n",
        "    'siz' : 11,          # 11, 23\n",
        "    'dr' : 0.03,           # 0.03, 0.01 ???0.005, 0.001\n",
        "    'k' : 9                   # 9, 12                     # TOTAAL = 2 * 2^2 * 2 * 2 * 2 * 2 = 128\n",
        "}\n",
        "\n",
        "build_best_transformer(data, epochs, params_best)\n",
        "\n",
        "# Grid search GRU\n",
        "params_rnn = {\n",
        "    'dropout' : [0.01, 0.005, 0.001]\n",
        "}\n",
        "\n",
        "grid_best_params_rnn = grid_search_rnn_and_calc_best(data, epochs_grid, params_rnn)\n",
        "\n",
        "\n",
        "# Build best GRU\n",
        "params_rnn_best = {\n",
        "    'dropout' : [0.001]\n",
        "}\n",
        "build_best_rnn(data, epochs, params_rnn_best)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHj80ZxLAZ1D"
      },
      "source": [
        "# Nieuwe sectie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZY8ztJx5RU0"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8AQyjaF0Q0n"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}